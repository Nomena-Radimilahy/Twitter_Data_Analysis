theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
vader <- read.csv("C:/Users/nomen/Twitter_Data_Analysis/Sentiment_analysis/twitter_vader.csv", sep = ",", header = T, stringsAsFactors = T)
head(vader)
str(vader)
vader <- vader %>%
mutate(Sentiment = case_when(
compound > 0.05 ~ 1,
compound < -0.05 ~ -1,
TRUE ~ 0
))
# Reorganisiere der Spalten
vader <- vader %>%
relocate(Sentiment, .after = compound)
# Write the data frame to a CSV file
write.csv(vader, file = "C:/Users/nomen/Twitter_Data_Analysis/Sentiment_analysis/vader_final.csv", row.names = FALSE)
vader$Sentiment <- factor(vader$Sentiment,
levels = c(1, 0, -1),
labels = c("positiv", "neutral", "negativ"))
sentiment_cases <- ggplot(vader) +
geom_bar(aes(x = Sentiment, fill = Sentiment)) +
geom_text(stat='count', aes(x=Sentiment, y=after_stat(count), label=after_stat(count)), vjust=-0.5) +
scale_fill_manual(values = c("positive" = "green4", "neutral" = "lightblue1", "negative" = "red1")) +
labs(title = "Sentiment zu ChatGPT auf Twitter",
x = "Sentiment",
y = "Anzahl")
# Berechnung der Prozentwerte
sentiment_summary <- as.data.frame(table(vader$Sentiment))
names(sentiment_summary) <- c("Sentiment", "Freq")
sentiment_summary$Percentage <- round((sentiment_summary$Freq / sum(sentiment_summary$Freq) * 100), 2)
# Add label position
sentiment_summary <- sentiment_summary %>%
arrange(desc(Sentiment)) %>%
mutate(lab.Percentage = cumsum(Freq) - 0.5*Freq)
farbe <- c("green4", "lightblue1", "red1" )
sentiment_pie <- ggplot(sentiment_summary, aes(x = "", y = Freq, fill = Sentiment)) +
geom_bar(width = 1, stat = "identity", color = "white") +
coord_polar("y", start = 0)+
geom_text(aes(y = lab.Percentage, label = Percentage), color = "white")+
scale_fill_manual(values = farbe) +
ggtitle("Sentimentverteilung (%)") +
theme_void()
plot_grid(sentiment_cases, sentiment_pie)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
suppressPackageStartupMessages({
library(dplyr)
library(lubridate)
library("RColorBrewer") # color pallets
library("ggplot2") # reportable graphs
library(cowplot) # arranges ggplot graphs nicely
library(moments) # for calculating skeweness
library(scales) # to change the format of the scales on the axis
})
# reading in the data
twitter <- read.csv("C:/Users/nomen/Twitter_Data_Analysis/data/twitter_cleaned.csv", sep = ",", header = T, stringsAsFactors = T)
head(twitter)
str(twitter)
# Fehlwerte nochmal checken
na_count <- sapply(twitter, function(x) sum(is.na(x)))
na_count
rows_with_na <- apply(twitter, 1, function(x) any(is.na(x)))
twitter[rows_with_na, ]
# Konvertieren der CreatedAt-Spalte in einen datetime-Typ
twitter$CreatedAt <- ymd_hms(twitter$CreatedAt)
# Gruppieren der Daten direkt nach Jahr und Monat und Zählen der Tweets pro Monat
tweets_per_month <- twitter %>%
group_by(YearMonth = floor_date(CreatedAt, "month")) %>%
summarise(Count = n())
# Balkendiagramm für die Kommentare pro Monat
ggplot(tweets_per_month, aes(x = YearMonth, y = Count)) +
geom_bar(stat = "identity", fill = "steelblue") +
geom_text(aes(label=Count), vjust=-0.5, hjust=1.5) +
labs(title = "Überblick über die Tweets pro Monat",
x = "Monat",
y = "Anzahl der Tweets") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
vader <- read.csv("C:/Users/nomen/Twitter_Data_Analysis/Sentiment_analysis/twitter_vader.csv", sep = ",", header = T, stringsAsFactors = T)
head(vader)
str(vader)
vader <- vader %>%
mutate(Sentiment = case_when(
compound > 0.05 ~ 1,
compound < -0.05 ~ -1,
TRUE ~ 0
))
# Reorganisiere der Spalten
vader <- vader %>%
relocate(Sentiment, .after = compound)
# Write the data frame to a CSV file
write.csv(vader, file = "C:/Users/nomen/Twitter_Data_Analysis/Sentiment_analysis/vader_final.csv", row.names = FALSE)
vader$Sentiment <- factor(vader$Sentiment,
levels = c(1, 0, -1),
labels = c("positiv", "neutral", "negativ"))
sentiment_cases <- ggplot(vader) +
geom_bar(aes(x = Sentiment, fill = Sentiment)) +
geom_text(stat='count', aes(x=Sentiment, y=after_stat(count), label=after_stat(count)), vjust=-0.5) +
scale_fill_manual(values = c("positive" = "green4", "neutral" = "lightblue1", "negative" = "red1")) +
labs(title = "Sentiment zu ChatGPT auf Twitter",
x = "Sentiment",
y = "Anzahl")
# Berechnung der Prozentwerte
sentiment_summary <- as.data.frame(table(vader$Sentiment))
names(sentiment_summary) <- c("Sentiment", "Freq")
sentiment_summary$Percentage <- round((sentiment_summary$Freq / sum(sentiment_summary$Freq) * 100), 2)
# Add label position
sentiment_summary <- sentiment_summary %>%
arrange(desc(Sentiment)) %>%
mutate(lab.Percentage = cumsum(Freq) - 0.5*Freq)
farbe <- c("green4", "lightblue1", "red1" )
sentiment_pie <- ggplot(sentiment_summary, aes(x = "", y = Freq, fill = Sentiment)) +
geom_bar(width = 1, stat = "identity", color = "white") +
coord_polar("y", start = 0)+
geom_text(aes(y = lab.Percentage, label = Percentage), color = "white")+
scale_fill_manual(values = farbe) +
ggtitle("Sentimentverteilung (%)") +
theme_void()
plot_grid(sentiment_cases, sentiment_pie)
vader$Sentiment <- factor(vader$Sentiment,
levels = c(1, 0, -1),
labels = c("positiv", "neutral", "negativ"))
sentiment_cases <- ggplot(vader) +
geom_bar(aes(x = Sentiment, fill = Sentiment)) +
geom_text(stat='count', aes(x=Sentiment, y=after_stat(count), label=after_stat(count)), vjust=-0.5) +
scale_fill_manual(values = c("positiv" = "green4", "neutral" = "lightblue1", "negativ" = "red1")) +
labs(title = "Sentiment zu ChatGPT auf Twitter",
x = "Sentiment",
y = "Anzahl")
# Berechnung der Prozentwerte
sentiment_summary <- as.data.frame(table(vader$Sentiment))
names(sentiment_summary) <- c("Sentiment", "Freq")
sentiment_summary$Percentage <- round((sentiment_summary$Freq / sum(sentiment_summary$Freq) * 100), 2)
# Add label position
sentiment_summary <- sentiment_summary %>%
arrange(desc(Sentiment)) %>%
mutate(lab.Percentage = cumsum(Freq) - 0.5*Freq)
farbe <- c("green4", "lightblue1", "red1" )
sentiment_pie <- ggplot(sentiment_summary, aes(x = "", y = Freq, fill = Sentiment)) +
geom_bar(width = 1, stat = "identity", color = "white") +
coord_polar("y", start = 0)+
geom_text(aes(y = lab.Percentage, label = Percentage), color = "white")+
scale_fill_manual(values = farbe) +
ggtitle("Sentimentverteilung (%)") +
theme_void()
plot_grid(sentiment_cases, sentiment_pie)
vader <- read.csv("C:/Users/nomen/Twitter_Data_Analysis/Sentiment_analysis/twitter_vader.csv", sep = ",", header = T, stringsAsFactors = T)
head(vader)
str(vader)
vader <- vader %>%
mutate(Sentiment = case_when(
compound > 0.05 ~ 1,
compound < -0.05 ~ -1,
TRUE ~ 0
))
# Reorganisiere der Spalten
vader <- vader %>%
relocate(Sentiment, .after = compound)
vader$Sentiment <- factor(vader$Sentiment,
levels = c(1, 0, -1),
labels = c("positiv", "neutral", "negativ"))
sentiment_cases <- ggplot(vader) +
geom_bar(aes(x = Sentiment, fill = Sentiment)) +
geom_text(stat='count', aes(x=Sentiment, y=after_stat(count), label=after_stat(count)), vjust=-0.5) +
scale_fill_manual(values = c("positiv" = "green4", "neutral" = "lightblue1", "negativ" = "red1")) +
labs(title = "Sentiment zu ChatGPT auf Twitter",
x = "Sentiment",
y = "Anzahl")
# Berechnung der Prozentwerte
sentiment_summary <- as.data.frame(table(vader$Sentiment))
names(sentiment_summary) <- c("Sentiment", "Freq")
sentiment_summary$Percentage <- round((sentiment_summary$Freq / sum(sentiment_summary$Freq) * 100), 2)
# Add label position
sentiment_summary <- sentiment_summary %>%
arrange(desc(Sentiment)) %>%
mutate(lab.Percentage = cumsum(Freq) - 0.5*Freq)
farbe <- c("green4", "lightblue1", "red1" )
sentiment_pie <- ggplot(sentiment_summary, aes(x = "", y = Freq, fill = Sentiment)) +
geom_bar(width = 1, stat = "identity", color = "white") +
coord_polar("y", start = 0)+
geom_text(aes(y = lab.Percentage, label = Percentage), color = "white")+
scale_fill_manual(values = farbe) +
ggtitle("Sentimentverteilung (%)") +
theme_void()
plot_grid(sentiment_cases, sentiment_pie)
vader$UserVerified <- factor(vader$UserVerified,
levels = c(1,0),
labels = c("verified", "not verified"))
sentiment_user <- ggplot(vader, aes(x = Sentiment, fill = UserVerified)) +
geom_bar(position = "dodge", width = 0.6) +
geom_text(stat='count', aes(group=UserVerified, label=after_stat(count)), vjust=-0.5, position=position_dodge(width=0.6)) +
scale_fill_manual(values = c("verified" = "dodgerblue4", "not verified" = "coral")) +
labs(title = "Sentiment of ChatGPT on Twitter",
x = "Sentiment",
y = "Count",
fill = "User Status") + # Legende für "fill" benennen
theme(plot.title = element_text(color = "royalblue4", size = 12, face = "bold"),
plot.caption = element_text(face = "italic"))
sentiment_user
vader$UserVerified <- factor(vader$UserVerified,
levels = c(1,0),
labels = c("geprüftes Konto", "ungeprüftes Konto"))
sentiment_user <- ggplot(vader, aes(x = Sentiment, fill = UserVerified)) +
geom_bar(position = "dodge", width = 0.6) +
geom_text(stat='count', aes(group=UserVerified, label=after_stat(count)), vjust=-0.5, position=position_dodge(width=0.6)) +
scale_fill_manual(values = c("geprüftes Konto" = "dodgerblue4", "ungeprüftes Konto" = "coral")) +
labs(title = "Sentimentverteilung pro user",
x = "Sentiment",
y = "Anzahl",
fill = "User") + # Legende für "fill" benennen
theme(plot.title = element_text(color = "royalblue4", size = 12, face = "bold"),
plot.caption = element_text(face = "italic"))
sentiment_user
vader <- read.csv("C:/Users/nomen/Twitter_Data_Analysis/Sentiment_analysis/twitter_vader.csv", sep = ",", header = T, stringsAsFactors = T)
head(vader)
str(vader)
vader <- vader %>%
mutate(Sentiment = case_when(
compound > 0.05 ~ 1,
compound < -0.05 ~ -1,
TRUE ~ 0
))
# Reorganisiere der Spalten
vader <- vader %>%
relocate(Sentiment, .after = compound)
vader$Sentiment <- factor(vader$Sentiment,
levels = c(1, 0, -1),
labels = c("positiv", "neutral", "negativ"))
sentiment_cases <- ggplot(vader) +
geom_bar(aes(x = Sentiment, fill = Sentiment)) +
geom_text(stat='count', aes(x=Sentiment, y=after_stat(count), label=after_stat(count)), vjust=-0.5) +
scale_fill_manual(values = c("positiv" = "green4", "neutral" = "lightblue1", "negativ" = "red1")) +
labs(title = "Sentiment zu ChatGPT auf Twitter",
x = "Sentiment",
y = "Anzahl")
# Berechnung der Prozentwerte
sentiment_summary <- as.data.frame(table(vader$Sentiment))
names(sentiment_summary) <- c("Sentiment", "Freq")
sentiment_summary$Percentage <- round((sentiment_summary$Freq / sum(sentiment_summary$Freq) * 100), 2)
# Add label position
sentiment_summary <- sentiment_summary %>%
arrange(desc(Sentiment)) %>%
mutate(lab.Percentage = cumsum(Freq) - 0.5*Freq)
farbe <- c("green4", "lightblue1", "red1" )
sentiment_pie <- ggplot(sentiment_summary, aes(x = "", y = Freq, fill = Sentiment)) +
geom_bar(width = 1, stat = "identity", color = "white") +
coord_polar("y", start = 0)+
geom_text(aes(y = lab.Percentage, label = Percentage), color = "white")+
scale_fill_manual(values = farbe) +
ggtitle("Sentimentverteilung (%)") +
theme_void()
plot_grid(sentiment_cases, sentiment_pie)
vader$UserVerified <- factor(vader$UserVerified,
levels = c(1,0),
labels = c("geprüftes Konto", "ungeprüftes Konto"))
sentiment_user <- ggplot(vader, aes(x = Sentiment, fill = UserVerified)) +
geom_bar(position = "dodge", width = 0.6) +
geom_text(stat='count', aes(group=UserVerified, label=after_stat(count)), vjust=-0.5, position=position_dodge(width=0.6)) +
scale_fill_manual(values = c("geprüftes Konto" = "dodgerblue4", "ungeprüftes Konto" = "coral")) +
labs(title = "Sentimentverteilung pro user",
x = "Sentiment",
y = "Anzahl",
fill = "User") + # Legende für "fill" benennen
theme(plot.title = element_text(color = "royalblue4", size = 12, face = "bold"),
plot.caption = element_text(face = "italic"))
sentiment_user
vader$CreatedAt <- as.Date(vader$CreatedAt, format = "%Y-%m-%d")
df_summary <- vader %>%
dplyr::group_by(CreatedAt, Sentiment) %>%
dplyr::summarise(Count = n(), .groups = 'drop')
ggplot(df_summary, aes(x = CreatedAt, y = Count, color = Sentiment, group = Sentiment)) +
geom_line() +
labs(x = "Datum", y = "Anzahl von Tweets", title = "Sentimententwicklung zu ChatGPT über die Zeit") +
theme_minimal() +
scale_color_manual(values = c("negativ" = "red", "neutral" = "blue", "positiv" = "green4")) +
theme(legend.title = element_blank())
vader$CreatedAt <- as.Date(vader$CreatedAt, format = "%Y-%m-%d")
df_summary <- vader %>%
dplyr::group_by(CreatedAt, Sentiment) %>%
dplyr::summarise(Count = n(), .groups = 'drop')
ggplot(df_summary, aes(x = CreatedAt, y = Count, color = Sentiment, group = Sentiment)) +
geom_line() +
labs(x = "Datum", y = "Anzahl von Tweets", title = "Sentimententwicklung zu ChatGPT über die Zeit") +
theme_minimal() +
scale_color_manual(values = c("negativ" = "red", "neutral" = "blue", "positiv" = "green4")) +
theme(legend.title = element_blank()) +
axis.line = element_line(colour = "black", size = 1, linetype = "solid"))
vader$CreatedAt <- as.Date(vader$CreatedAt, format = "%Y-%m-%d")
df_summary <- vader %>%
dplyr::group_by(CreatedAt, Sentiment) %>%
dplyr::summarise(Count = n(), .groups = 'drop')
ggplot(df_summary, aes(x = CreatedAt, y = Count, color = Sentiment, group = Sentiment)) +
geom_line() +
labs(x = "Datum", y = "Anzahl von Tweets", title = "Sentimententwicklung zu ChatGPT über die Zeit") +
theme_minimal() +
scale_color_manual(values = c("negativ" = "red", "neutral" = "blue", "positiv" = "green4")) +
theme(legend.title = element_blank()) , axis.line = element_line(colour = "black", size = 1, linetype = "solid"))
vader$CreatedAt <- as.Date(vader$CreatedAt, format = "%Y-%m-%d")
df_summary <- vader %>%
dplyr::group_by(CreatedAt, Sentiment) %>%
dplyr::summarise(Count = n(), .groups = 'drop')
ggplot(df_summary, aes(x = CreatedAt, y = Count, color = Sentiment, group = Sentiment)) +
geom_line() +
labs(x = "Datum", y = "Anzahl von Tweets", title = "Sentimententwicklung zu ChatGPT über die Zeit") +
theme_minimal() +
scale_color_manual(values = c("negativ" = "red", "neutral" = "blue", "positiv" = "green4")) +
theme(legend.title = element_blank()) +
axis.line = element_line(colour = "black", size = 1, linetype = "solid"))
vader$CreatedAt <- as.Date(vader$CreatedAt, format = "%Y-%m-%d")
df_summary <- vader %>%
dplyr::group_by(CreatedAt, Sentiment) %>%
dplyr::summarise(Count = n(), .groups = 'drop')
ggplot(df_summary, aes(x = CreatedAt, y = Count, color = Sentiment, group = Sentiment)) +
geom_line() +
labs(x = "Datum", y = "Anzahl von Tweets", title = "Sentimententwicklung zu ChatGPT über die Zeit") +
theme_minimal() +
scale_color_manual(values = c("negativ" = "red", "neutral" = "blue", "positiv" = "green4")) +
theme(legend.title = element_blank(),
axis.line = element_line(colour = "black", size = 1, linetype = "solid"))
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
# Laden der benötigten Bibliothek
suppressPackageStartupMessages({
library(lubridate)
library(dplyr)
})
# Datensatz einlesen
twitter <- read.csv("C:/Users/nomen/Twitter_Data_Analysis/data/twitter_data.csv", sep = ",", header = T, stringsAsFactors = T)
str(twitter)
# Konvertieren der Spalte "CreatedAt" in ein Datumsformat
twitter$CreatedAt <- as.POSIXct(twitter$CreatedAt, format = "%Y-%m-%d %H:%M:%S")
# Filtern der Daten
twitter <- twitter %>%
filter(CreatedAt >= as.POSIXct("2022-11-30 00:00:00") & CreatedAt <= as.POSIXct("2023-02-06 23:59:59"))
twitter <- twitter[!startsWith(as.character(twitter$Text), "RT"), ]
# Define the keywords you want to search for
keywords <- c("openai", "open_ai", " ai ", " AI ", "KI", "ki", "chatbot",
"artificial intelligence", "chatgpt", "GPT", "GPT-2", "GPT-3", "tool", "natural language", "language")
# Create a pattern string for grepl
pattern <- paste(keywords, collapse = "|")
# Filter tweets that contain any of the keywords
relevant_tweets <- twitter %>%
filter(grepl(pattern, Text, ignore.case = TRUE))
# View the filtered tweets
head(relevant_tweets)
twitter_new <- data.frame(Id = relevant_tweets$Id, CreatedAt = relevant_tweets$CreatedAt, RetweetCount = relevant_tweets$RetweetCount, FavoriteCount...LikeCount = relevant_tweets$FavoriteCount...LikeCount, Text = relevant_tweets$Text, UserFollowersCount = relevant_tweets$UserFollowersCount, UserFriendsCount = relevant_tweets$UserFriendsCount, UserVerified = relevant_tweets$UserVerified)
# explizite Fehlwerte
colSums((is.na(twitter_new)))
rows_with_na <- apply(twitter_new, 1, function(x) any(is.na(x)))
twitter_new[rows_with_na, ]
# implizite Fehlwerte
sapply(twitter_new, function(x) sum(x %in% c("", " ", "NA", "N/A", "unknown", "Not Available")))
# Set the display options to show full numeric values
options(scipen = 999)
str(twitter_new)
#View(twitter_clean)
# Write the data frame to a CSV file
write.csv(twitter_new, file = "C:/Users/nomen/Twitter_Data_Analysis/data/twitter_clean.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
View(twitter)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
# Laden der benötigten Bibliothek
suppressPackageStartupMessages({
library(lubridate)
library(dplyr)
})
# Datensatz einlesen
twitter <- read.csv("C:/Users/nomen/Twitter_Data_Analysis/data/twitter_data.csv", sep = ",", header = T, stringsAsFactors = T)
str(twitter)
# Konvertieren der Spalte "CreatedAt" in ein Datumsformat
twitter$CreatedAt <- as.POSIXct(twitter$CreatedAt, format = "%Y-%m-%d %H:%M:%S")
# Filtern der Daten
twitter <- twitter %>%
filter(CreatedAt >= as.POSIXct("2022-11-30 00:00:00") & CreatedAt <= as.POSIXct("2023-02-06 23:59:59"))
twitter <- twitter[!startsWith(as.character(twitter$Text), "RT"), ]
View(twitter)
# Define the keywords you want to search for
keywords <- c("openai", "open_ai", " ai ", " AI ", "KI", "ki", "chatbot",
"artificial intelligence", "chatgpt", "GPT", "GPT-2", "GPT-3", "tool", "natural language", "language")
# Create a pattern string for grepl
pattern <- paste(keywords, collapse = "|")
# Filter tweets that contain any of the keywords
relevant_tweets <- twitter %>%
filter(grepl(pattern, Text, ignore.case = TRUE))
# View the filtered tweets
head(relevant_tweets)
# Filter tweets that do not contain any of the keywords
irrelevant_tweets <- twitter %>%
filter(!grepl(pattern, Text, ignore.case = TRUE))
# View the filtered tweets
head(irrelevant_tweets)
View(irrelevant_tweets)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
# Laden der benötigten Bibliothek
suppressPackageStartupMessages({
library(lubridate)
library(dplyr)
})
# Datensatz einlesen
twitter <- read.csv("C:/Users/nomen/Twitter_Data_Analysis/data/twitter_data.csv", sep = ",", header = T, stringsAsFactors = T)
str(twitter)
# Konvertieren der Spalte "CreatedAt" in ein Datumsformat
twitter$CreatedAt <- as.POSIXct(twitter$CreatedAt, format = "%Y-%m-%d %H:%M:%S")
# Filtern der Daten
twitter <- twitter %>%
filter(CreatedAt >= as.POSIXct("2022-11-30 00:00:00") & CreatedAt <= as.POSIXct("2023-02-06 23:59:59"))
twitter <- twitter[!startsWith(as.character(twitter$Text), "RT"), ]
# Define the keywords you want to search for
keywords <- c("openai", "open_ai", " ai ", " AI ", "KI", "ki", "chatbot",
"artificial intelligence", "chatgpt", "GPT", "GPT-2", "GPT-3", "tool", "natural language", "language")
# Create a pattern string for grepl
pattern <- paste(keywords, collapse = "|")
# Filter tweets that contain any of the keywords
relevant_tweets <- twitter %>%
filter(grepl(pattern, Text, ignore.case = TRUE))
# View the filtered tweets
head(relevant_tweets)
# Filter tweets that do not contain any of the keywords
irrelevant_tweets <- twitter %>%
filter(!grepl(pattern, Text, ignore.case = TRUE))
# View the filtered tweets
head(irrelevant_tweets)
twitter_new <- data.frame(Id = relevant_tweets$Id, CreatedAt = relevant_tweets$CreatedAt, RetweetCount = relevant_tweets$RetweetCount, FavoriteCount...LikeCount = relevant_tweets$FavoriteCount...LikeCount, Text = relevant_tweets$Text, UserFollowersCount = relevant_tweets$UserFollowersCount, UserFriendsCount = relevant_tweets$UserFriendsCount, UserVerified = relevant_tweets$UserVerified)
# explizite Fehlwerte
colSums((is.na(twitter_new)))
rows_with_na <- apply(twitter_new, 1, function(x) any(is.na(x)))
twitter_new[rows_with_na, ]
# implizite Fehlwerte
sapply(twitter_new, function(x) sum(x %in% c("", " ", "NA", "N/A", "unknown", "Not Available")))
# Set the display options to show full numeric values
options(scipen = 999)
str(twitter_new)
#View(twitter_clean)
# Write the data frame to a CSV file
write.csv(twitter_new, file = "C:/Users/nomen/Twitter_Data_Analysis/data/twitter_clean.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
suppressPackageStartupMessages({
library(dplyr)
library(lubridate)
library("RColorBrewer") # color pallets
library("ggplot2") # reportable graphs
library(cowplot) # arranges ggplot graphs nicely
library(moments) # for calculating skeweness
library(scales) # to change the format of the scales on the axis
})
# reading in the data
twitter <- read.csv("C:/Users/nomen/Twitter_Data_Analysis/data/twitter_cleaned.csv", sep = ",", header = T, stringsAsFactors = T)
head(twitter)
str(twitter)
# Fehlwerte nochmal checken
na_count <- sapply(twitter, function(x) sum(is.na(x)))
na_count
rows_with_na <- apply(twitter, 1, function(x) any(is.na(x)))
twitter[rows_with_na, ]
# Konvertieren der CreatedAt-Spalte in einen datetime-Typ
twitter$CreatedAt <- ymd_hms(twitter$CreatedAt)
# Gruppieren der Daten direkt nach Jahr und Monat und Zählen der Tweets pro Monat
tweets_per_month <- twitter %>%
group_by(YearMonth = floor_date(CreatedAt, "month")) %>%
summarise(Count = n())
# Balkendiagramm für die Kommentare pro Monat
ggplot(tweets_per_month, aes(x = YearMonth, y = Count)) +
geom_bar(stat = "identity", fill = "steelblue") +
geom_text(aes(label=Count), vjust=-0.5, hjust=1.5) +
labs(title = "Überblick über die Tweets pro Monat",
x = "Monat",
y = "Anzahl der Tweets") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
# Laden der benötigten Bibliothek
suppressPackageStartupMessages({
library(lubridate)
library(dplyr)
})
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
# Laden der benötigten Bibliothek
suppressPackageStartupMessages({
library(lubridate)
library(dplyr)
})
# Datensatz einlesen
twitter <- read.csv("C:/Users/nomen/Twitter_Data_Analysis/data/twitter_data.csv", sep = ",", header = T, stringsAsFactors = T)
str(twitter)
# Konvertieren der Spalte "CreatedAt" in ein Datumsformat
twitter$CreatedAt <- as.POSIXct(twitter$CreatedAt, format = "%Y-%m-%d %H:%M:%S")
# Filtern der Daten
twitter <- twitter %>%
filter(CreatedAt >= as.POSIXct("2022-11-30 00:00:00") & CreatedAt <= as.POSIXct("2023-02-06 23:59:59"))
twitter <- twitter[!startsWith(as.character(twitter$Text), "RT"), ]
# Define the keywords you want to search for
keywords <- c("openai", "open_ai", " ai ", " AI ", "KI", "ki", "chatbot",
"artificial intelligence", "chatgpt", "GPT", "GPT-2", "GPT-3", "tool", "natural language", "language")
# Create a pattern string for grepl
pattern <- paste(keywords, collapse = "|")
# Filter tweets that contain any of the keywords
relevant_tweets <- twitter %>%
filter(grepl(pattern, Text, ignore.case = TRUE))
# View the filtered tweets
head(relevant_tweets)
# Filter tweets that do not contain any of the keywords
irrelevant_tweets <- twitter %>%
filter(!grepl(pattern, Text, ignore.case = TRUE))
# View the filtered tweets
head(irrelevant_tweets)
twitter_new <- data.frame(Id = relevant_tweets$Id, CreatedAt = relevant_tweets$CreatedAt, RetweetCount = relevant_tweets$RetweetCount, FavoriteCount...LikeCount = relevant_tweets$FavoriteCount...LikeCount, Text = relevant_tweets$Text, UserFollowersCount = relevant_tweets$UserFollowersCount, UserFriendsCount = relevant_tweets$UserFriendsCount, UserVerified = relevant_tweets$UserVerified)
# explizite Fehlwerte
colSums((is.na(twitter_new)))
rows_with_na <- apply(twitter_new, 1, function(x) any(is.na(x)))
twitter_new[rows_with_na, ]
# implizite Fehlwerte
sapply(twitter_new, function(x) sum(x %in% c("", " ", "NA", "N/A", "unknown", "Not Available")))
twitter_new[is.na(CreatedAt)]
is.na(twitter_new$CreatedAt)
